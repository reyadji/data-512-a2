{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2 Data Bias\n",
    "\n",
    "The goal of this assignment is to detect bias on political figures from multiple countries by analyzing of the *coverage* and the *quality* of the the politican's Wikipedia articles. The main output of this project is a data visualization of highest-ranked and lowest-ranked countries in term of politican article density. The visuals itself does not tell the entire story, but it hopefully helps uncovering political biases in Wikipedia. \n",
    "\n",
    "To support the analysis, I produce these artifacts:\n",
    "1. A CSV file with politicans Wikipedia article information (country, article name, revision ID, article quality, and population of the country)\n",
    "2. A Jupyter notebook with all the code\n",
    "3. A comprehensive README file\n",
    "4. A MIT LICENSE file\n",
    "5. A visualization png file\n",
    "\n",
    "To be able to effectively process and analyze the data, this project requires:\n",
    "1. Politicians Wikipedia dataset \n",
    "2. Population data of each country\n",
    "3. Article quality prediction by Wikimedia ORES machine learning service\n",
    "\n",
    "these will be downloaded, requested, combined, plotted, and eventually analyzed in this Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Library\n",
    "\n",
    "Setting up shared import and variables to be used throughout this Jupyter notebook. This project's requirements are pandas and requests Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "\n",
    "# Variables\n",
    "my_github = 'reyadji'\n",
    "my_email = 'adjir@uw.edu'\n",
    "ores_url = 'https://ores.wikimedia.org/v3/scores/enwiki'\n",
    "page_file = 'page_data.csv'\n",
    "population_file = 'Population Mid-2015.csv'\n",
    "project = 'enwiki'\n",
    "model = 'wp10'\n",
    "csv_file = 'A2 data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Article & Population Data\n",
    "\n",
    "The article dataset CSV file can be downloaded on the figshare(https://figshare.com/articles/Untitled_Item/5513449), while the population dataset CSV file is available on the Population Research Bureau website (http://www.prb.org/DataFinder/Topic/Rankings.aspx?ind=14). This step is pretty straight-forward as long as the correct dataset are downloaded (the try-catch is to make sure it is available in the current directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading page_data and population csv files to pandas dataframe inside try-catch block\n",
    "try: \n",
    "    page_data = pd.read_csv(page_file)\n",
    "    pop_data = pd.read_csv(population_file, header=1, thousands=',')\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "# print(page_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting article quality predictions and Combining the Datasets\n",
    "\n",
    "Each Wikipedia article is classified into one of 6 categories by ORES (Objective Revision Evaluation Service) machine learning service. These categoris from best to worst are:\n",
    "FA: Featured article\n",
    "GA: Good article\n",
    "B: B-class article\n",
    "C: C-class article\n",
    "Start: Start-class article\n",
    "Stub: Stub-class article\n",
    "\n",
    "ORES predicts probability number of each category for every article, and it picks category with the highest probability as the article's category. Fortunately, ORES has a public ReST API that accepts an article's revision ID or a batch of articles' revision IDs (up to 140), and it returns the prediction value of said article with a the full set of probability for each category.\n",
    "\n",
    "After being loaded, each dataset is cleaned and combined by dropping unnecessary columns (Location Type, TimeFrame, Data Type, Foornotes from population dataset), by renaming remaining columns to the specification, and by being inner-joined on *country* attribute (orphaned rows are filtered out). \n",
    "\n",
    "The cleaned dataset is split into subsets of 100-rows since ORES API only takes roughly up to 140 revision IDs in one single batch request. The resulting article quality dataset is then merged to the main dataset on the *revision_id* before everything is stored into a CSV file.\n",
    "\n",
    "Manual testing is added at the end to check the integrity of the dataset as well as its accuracy (I chose Indonesia since it's my birth country and I am pretty familiar with the names of the politician). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not get a valid article quality for revision id:807367030, article:46862    Jalal Movaghar\n",
      "Name: article_name, dtype: object\n",
      "Could not get a valid article quality for revision id:807367166, article:46863    Mohsen Movaghar\n",
      "Name: article_name, dtype: object\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'A2 data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9572eccec425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Merge the article quality dataframe with the main dataframe and stored to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maq_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'revision_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/Documents/MSDS/msdsenv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1401\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/Documents/MSDS/msdsenv/lib/python3.6/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1576\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/Documents/MSDS/msdsenv/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'A2 data.csv'"
     ]
    }
   ],
   "source": [
    "# Cleaning and processing both dataframes before joining them on country\n",
    "page_data.rev_id = page_data.rev_id.apply(str)\n",
    "page_data = page_data.rename(columns = {'page': 'article_name', 'rev_id': 'revision_id'})\n",
    "pop_data = pop_data.drop(['Location Type','TimeFrame', 'Data Type', 'Footnotes'], axis=1)\n",
    "pop_data = pop_data.rename(columns = {'Location': 'country', 'Data': 'population'})\n",
    "data = pd.merge(page_data, pop_data, on='country', how='inner')\n",
    "\n",
    "# Subset the dataframe to 100-row chunks\n",
    "num_of_subsets = math.ceil(len(data)/100)\n",
    "subsets = np.array_split(data, num_of_subsets)\n",
    "\n",
    "# Processing each subset to the ORES API to get quality\n",
    "aq_df = pd.DataFrame()\n",
    "for s in subsets:\n",
    "    rev_ids = s.revision_id.str.cat(sep='|')\n",
    "    headers = {\n",
    "        'User-Agent': 'https://github.com/{}'.format(my_github), \n",
    "        'From': my_email}\n",
    "    payload = {\n",
    "        'models': 'wp10',\n",
    "        'revids': rev_ids\n",
    "    }\n",
    "    r = requests.get(ores_url, params=payload, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print ('Response status code is 200. Response text: {}'.format(r.text))\n",
    "        continue\n",
    "    revision_id = []\n",
    "    article_quality = []\n",
    "    \n",
    "    for k,v in r.json()['enwiki']['scores'].items():\n",
    "        try:\n",
    "            pred = v['wp10']['score']['prediction']\n",
    "            revision_id.append(k)\n",
    "            article_quality.append(pred)\n",
    "        except KeyError:\n",
    "            print('Could not get a valid article quality for revision id:{0}, article:{1}'.format(k, page_data.loc[page_data.revision_id == k].article_name))\n",
    "            continue\n",
    "    aq_df = pd.concat([aq_df, pd.DataFrame(data={'revision_id': revision_id, 'article_quality':article_quality})]) \n",
    "\n",
    "# Merge the article quality dataframe with the main dataframe and stored to CSV\n",
    "data = pd.merge(data, aq_df, on='revision_id', how='inner')\n",
    "data.to_csv(csv_file, index = False)\n",
    "\n",
    "# Testing\n",
    "print('Dataset shape: {}'.format(data.shape))\n",
    "print('Indonesian politicans: {}'.format(data.loc[(data.country =='Indonesia')].article_name.sample(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "In the analysis part, we have 2 metrics to calculate:\n",
    "1. The proportion of the number of each country articles per its population per country.\n",
    "2. The percentage of high-quality articles (either FA or GA article) over total articles.\n",
    "Using these two metrics, I hopefully am able to prove or disprove the notion there is biases toward certain country politicians in English Wikipedia.\n",
    "\n",
    "Proportion of articles per country population is calculated by dividing the number of articles of each country with the country population. This can be achieved by grouping the dataset by its *country* and summing the number of the articles. This new groupby dataset is divided by the country's *population* from the population dataset. Finding the highest and lowest proportion is only a matter of sorting.\n",
    "\n",
    "Calculating high-quality articles are a little trickier since I need to define a boolean function to determine whether each article a high-quality one or not. Aside from that extra function, the number of high-quality articles proportioned to the number of all articles is straight-forward.\n",
    "\n",
    "To test this analysis module, I select all Indonesian politicians with high-quality articles. The number is disappointingly very low in proportion to population of Indonesia. Countries with high and low proportions will be discussed in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   article_name    country revision_id  population  \\\n",
      "7533                    Sukarni  Indonesia   781071895   255741973   \n",
      "7596  Alexander Andries Maramis  Indonesia   798756726   255741973   \n",
      "7599               Khouw Kim An  Indonesia   798837376   255741973   \n",
      "7601        Maria Ulfah Santoso  Indonesia   798891601   255741973   \n",
      "7610      Sri Mulyani Indrawati  Indonesia   800672774   255741973   \n",
      "7629            Teuku Nyak Arif  Indonesia   802855428   255741973   \n",
      "7630       Abdul Haris Nasution  Indonesia   803287932   255741973   \n",
      "7650                 Rano Karno  Indonesia   805553992   255741973   \n",
      "7659        Korrie Layun Rampan  Indonesia   807049212   255741973   \n",
      "\n",
      "     article_quality high_quality  \n",
      "7533              GA         True  \n",
      "7596              GA         True  \n",
      "7599              GA         True  \n",
      "7601              GA         True  \n",
      "7610              GA         True  \n",
      "7629              GA         True  \n",
      "7630              GA         True  \n",
      "7650              GA         True  \n",
      "7659              GA         True  \n"
     ]
    }
   ],
   "source": [
    "# Grouping the number of articles by the country, and merged it with population dataset\n",
    "pop_data = pop_data.set_index('country')\n",
    "country_df = pd.concat(\n",
    "    [pd.DataFrame({'articles':data.groupby('country').size()}), pop_data], \n",
    "    axis=1, \n",
    "    join='inner')\n",
    "country_df = country_df.assign(art_per_country=country_df.articles*100/country_df.population)\n",
    "# country_df = country_df.sort_values('art_per_country')\n",
    "\n",
    "# Define a function to count all articles with 'FA' or 'GA' quality\n",
    "def is_highquality(article):\n",
    "    return True if (article == 'FA' or article == 'GA') else False\n",
    "\n",
    "# Apply the above function to article_quality column to get boolean high_quality column\n",
    "data['high_quality']= data.article_quality.apply(is_highquality)\n",
    "hq_df = data.groupby('country')['high_quality'].sum()\n",
    "\n",
    "# Join the high_quality dataset with the previous one and calculate the proportion of high quality articles\n",
    "country_df = country_df.join(hq_df)\n",
    "country_df['high_quality_prop'] = country_df.high_quality*100/country_df.articles\n",
    "\n",
    "# Testing\n",
    "print(data.loc[(data.country =='Indonesia') & (data.high_quality)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "This section is to visualize the 10 highest-ranked and lowest-ranked countries in term of number of politician articles as proportion of country population and the number of high-quality articles as proportion of all articles. Before I start any plotting, I need to make sure I the countries are ranked appropriately. There are 39 countries with 0 high-quality articles; these countries are tied up for the last place in high-quality articles per total articles rank.\n",
    "While having the lowest and highest-ranked countries in the same bar chart could be useful, this is not the case since the lowest-ranked countries have much smaller articles-per-population than the highest-ranked. Hence, I split the lowest-ranked countries and highest-ranked countries in different plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'country_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3175ffb62fbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlow_apc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountry_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'art_per_country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlow_apc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mart_per_country\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# Articles per Population (\\%)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'country_df' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Plotting highest-ranked and lowest-ranked countries in term of number of politician articles per population\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "low_apc = country_df.sort_values('art_per_country').head(10)\n",
    "low_apc.art_per_country.plot.bar(color='r')\n",
    "ax1.set_ylabel('# Articles per Population (\\%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "high_apc = country_df.sort_values('art_per_country').tail(10)\n",
    "high_apc.art_per_country.plot.bar(color='b')\n",
    "ax2.set_ylabel('# Articles per Population (\\%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig.suptitle('The 10 lowest-ranked and highest-ranked countries in terms of number of politician articles as a proportion of country population')\n",
    "plt.show()\n",
    "fig.savefig('articles_per_population.png')\n",
    "\n",
    "# Plotting highest-ranked and lowest-ranked countries in term of number of high-quality articles per total articles\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax3 = fig.add_subplot(121)\n",
    "low_hqa = country_df.loc[country_df.high_quality!=0].sort_values('high_quality_prop').head(9)\n",
    "low_hqa = pd.concat([pd.DataFrame({'high_quality': 0.0, 'high_quality_prop': 0.0}, index=['Misc.']), low_hqa])\n",
    "low_hqa.high_quality_prop.plot.bar(color='r')\n",
    "ax3.set_ylabel('# High Quality Articles per Total Articles (\\%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "ax4 = fig.add_subplot(122)\n",
    "high_hqa = country_df.loc[country_df.high_quality!=0].sort_values('high_quality_prop').tail(10)\n",
    "high_hqa.high_quality_prop.plot.bar(color='b')\n",
    "ax4.set_ylabel('# High Quality Articles per Total Articles (\\%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.suptitle('The 10 lowest-ranked and highest-ranked countries in terms of number of GA and FA-quality articles as a proportion of all articles about politicians from that country')\n",
    "plt.show()\n",
    "fig.savefig('high_quality_articles_proportion.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### number of politician articles as a proportion of country population\n",
    "The 10 lowest-ranked country in terms of number of politician articles as a proportion of country population in Wikipedia is dominated by countries with big population (China, India, Indonesia) or by countries where English-speaking editors are less familiar with, such as Bangladesh, Zambia or by countries with closed political system, like North Korea. On the other hand, the top 10 country in terms of number of politician articles as a proportion of country population consists exclusively of countries with very small population. While this metric sheds a light on  bias against politicians from less-familiar countries in Wikipedia, I found the metric/analysis itself has inherently flawed against countries with big population and vice versa. It is by no means an indication that English-speaking Wikipedia biased against China, India, and Indonesia. Afterall, China(1138 articles), India(990 articles), and Indonesia(215 articles) have decent number of total articles in English Wikipedia, and there is no linear correlation between total number of articles and population.\n",
    "\n",
    "### number of high-quality articles as a proportion of all articles about politicians from that country\n",
    "The second metric filters out \"placeholder\" and \"empty calories\" articles about politicians to reduce the noise. The hypotheses is biases would be more pronounced when there is only a handful relevant articles. However, there is no obvious bias against the lowest-ranked countries in this metric (excluding 39 countries without good or featured article). It consists of countries from 4 different continents with various population (Nigeria has the largest population, tiny Luxembourg has the smalles one), developed economically (Finland, Luxembourg) or developing economically (Nigeria, Czech Republic). The only similarities among these countries is English is not the primary or even secondary everyday language. The other end of the spectrum also shows very little bias. Although United States is in the top 10, it does not even have the highest proportion of well-written articles to all articles. That distinction surprisingly belongs to North Korea by a wide margin!! 9 out of 39 articles about the Hermit Kingdom's politicians are deemed good or featured article. It is even more surprising considering North Korea is in the bottom 10 of number of politician articles as proportion of country population since there is not that many articles. I am under the impression that a North Korean politic expert (or a group of researchers) is dedicated to write most, if not all, of North Korea politician articles. A cursory glance at the articles' edit history confirm that WIkiproject North Korea makes significant contribution to the North Korean-related Wikipedia articles.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
